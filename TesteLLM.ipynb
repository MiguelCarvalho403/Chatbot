{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ab09af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.08"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_size = 1.7\n",
    "Q_size = 16\n",
    "\n",
    "m = 1.2*(model_size*4)/(32/Q_size)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a138cf-12de-4b85-9580-a76f2a555a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qwen_32 = \"Qwen/Qwen3-1.7B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6525e957",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4846b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2b29dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/miniforge3/envs/DataScience/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16854980-f8da-43ea-a099-dcfd80648373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.39it/s]\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "qwen = pipeline(\"text-generation\", model=model_qwen, device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56873bb2-7e48-4992-83e7-8ad166d6bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = qwen(\"Quem descobriu o Brasil?\", max_new_tokens=100, do_sample=True, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fe08bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Quem descobriu o Brasil? Quem foi o primeiro colonizador? Quem foi o primeiro governador? Quem foi o primeiro presidente? Quem foi o primeiro presidente da República? Quem foi o primeiro presidente da República? Quem foi o primeiro presidente da República? Quem foi o primeiro presidente da República? Quem foi o primeiro presidente da República? Quem foi o primeiro presidente da República? Quem foi o primeiro presidente da República? Quem foi o primeiro presidente da República? Quem foi o primeiro'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668c079e",
   "metadata": {},
   "source": [
    "## LangChain_HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df3c9ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b51765fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.74s/it]\n",
      "Device has 1 GPUs available. Provide device={deviceId} to `from_model_id` to use availableGPUs for execution. deviceId is -1 (default) for CPU and can be a positive integer associated with CUDA device id.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_qwen,\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 150, \"do_sample\": True, \"temperature\": 0.1, \"top_p\": 0.7, \"top_k\": 50, \"repetition_penalty\": 1.2, \n",
    "                     \"num_return_sequences\": 1},\n",
    "    device=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "842a17d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'top_k': 20}. If this is not desired, please set these values explicitly.\n"
     ]
    }
   ],
   "source": [
    "ans = hf.invoke(\"Quem voce é?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba8425f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quem voce é? Como você se sente?\n",
      "\n",
      "Eu sou um modelo de linguagem, desenvolvido pela Alibaba Group. Eu não tenho uma identidade pessoal ou sentimentos como seres humanos. Minha função principal é ajudar a pessoa com informações e respostas sobre diversos temas.\n",
      "\n",
      "Se eu quiser que o meu sistema respondesse em português, posso dizer isso no início do texto.\n",
      "Answer:\n",
      "Okay, the user is asking me to explain who I am and how I feel. But since I'm an AI model developed by Alibaba, I can't have personal feelings or identities like humans. My main purpose is to provide helpful information and answers on various topics.\n",
      "\n",
      "I need to make sure my response clearly states that I don\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e830f93c-9cce-45f5-89a0-abe687762581",
   "metadata": {},
   "source": [
    "## AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba8de574-84da-4c68-b65a-6f29485078b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a237080b-5e85-4df6-9114-b162ff5c0e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_qwen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0ec3782-8145-4b3d-b09d-e52b27026fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    40,   3003,   1012,   8580,    369,    264,    472,  35268,  16281,\n",
      "           3308,    847,   4361,   2272,     13],\n",
      "        [    40,  12213,    419,    773,   1753,      0, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b674fba9-e33a-49ba-aa50-cbb8d5938caa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
